{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c298b01-6a63-4ff2-876e-a697db93bd18",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92c4d4b8-7253-4035-84fb-a66b3e148004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "import numpy as np\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddcf2c2-af93-4eda-a166-7f657d6216f5",
   "metadata": {},
   "source": [
    "# Load dataset into variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0af9944-1274-4595-af8b-60122d20cb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_asteroid_data():\n",
    "    #csv_path = \"shuffled_asteroids_head.csv\"\n",
    "    csv_path = \"combined.csv\"\n",
    "    return pd.read_csv(csv_path, low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b3b06c-cf68-4b3f-ab51-8c43d6f356f4",
   "metadata": {},
   "source": [
    "# Removing unneeded features and null features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48e4de44-996b-4b03-a0b3-eb402663bc94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10027 entries, 0 to 10026\n",
      "Data columns (total 35 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   id         10027 non-null  object \n",
      " 1   neo        10027 non-null  object \n",
      " 2   pha        10027 non-null  object \n",
      " 3   H          10027 non-null  float64\n",
      " 4   epoch      10027 non-null  float64\n",
      " 5   epoch_mjd  10027 non-null  int64  \n",
      " 6   epoch_cal  10027 non-null  float64\n",
      " 7   e          10027 non-null  float64\n",
      " 8   a          10027 non-null  float64\n",
      " 9   q          10027 non-null  float64\n",
      " 10  i          10027 non-null  float64\n",
      " 11  om         10027 non-null  float64\n",
      " 12  w          10027 non-null  float64\n",
      " 13  ma         10027 non-null  float64\n",
      " 14  ad         10027 non-null  float64\n",
      " 15  n          10027 non-null  float64\n",
      " 16  tp         10027 non-null  float64\n",
      " 17  tp_cal     10027 non-null  float64\n",
      " 18  per        10027 non-null  float64\n",
      " 19  per_y      10027 non-null  float64\n",
      " 20  moid       10027 non-null  float64\n",
      " 21  moid_ld    10027 non-null  float64\n",
      " 22  sigma_e    10027 non-null  float64\n",
      " 23  sigma_a    10027 non-null  float64\n",
      " 24  sigma_q    10027 non-null  float64\n",
      " 25  sigma_i    10027 non-null  float64\n",
      " 26  sigma_om   10027 non-null  float64\n",
      " 27  sigma_w    10027 non-null  float64\n",
      " 28  sigma_ma   10027 non-null  float64\n",
      " 29  sigma_ad   10027 non-null  float64\n",
      " 30  sigma_n    10027 non-null  float64\n",
      " 31  sigma_tp   10027 non-null  float64\n",
      " 32  sigma_per  10027 non-null  float64\n",
      " 33  class      10027 non-null  object \n",
      " 34  rms        10027 non-null  float64\n",
      "dtypes: float64(30), int64(1), object(4)\n",
      "memory usage: 2.7+ MB\n"
     ]
    }
   ],
   "source": [
    "asteroids = load_asteroid_data()\n",
    "#asteroids.head()\n",
    "asteroids = asteroids.dropna(subset=['pha'])\n",
    "asteroids = asteroids.dropna(subset=['sigma_e'])\n",
    "asteroids = asteroids.dropna(subset=['neo'])\n",
    "asteroids = asteroids.dropna(subset=['H'])\n",
    "asteroids = asteroids.dropna(subset=['ma'])\n",
    "asteroids = asteroids.drop(columns=['spkid', 'full_name', 'pdes', 'prefix', 'name', 'orbit_id', 'equinox', 'diameter', \"diameter_sigma\", \"albedo\"])\n",
    "asteroids.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efc0bfc-6f92-4cd3-ba25-4d224f092d06",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "92dcfaed-b3cc-41f5-98ac-53ee0a590aba",
   "metadata": {},
   "source": [
    "# Converting object datatypes to numerical and standardizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88412f9f-6489-4447-8ca8-bcbb9bd58daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "std_scaler = StandardScaler()\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "\n",
    "#splitting features into numerical and categorical\n",
    "asteroids_num = asteroids.drop(columns=[\"id\", \"neo\", \"pha\", \"class\"])\n",
    "asteroids_id = asteroids[[\"id\"]]\n",
    "asteroids_cat = asteroids[[\"neo\", \"pha\", \"class\"]]\n",
    "\n",
    "#encode categorical features\n",
    "asteroid_cat_encoded = ordinal_encoder.fit_transform(asteroids_cat)\n",
    "\n",
    "#Normalizing numerical features\n",
    "std_scaler = StandardScaler()\n",
    "asteroid_num_scaled = std_scaler.fit_transform(asteroids_num)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde35eab-553d-45f9-b96d-8d6d39ec9468",
   "metadata": {},
   "source": [
    "# Creating features and labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0918a8a-cfdf-4cc9-8f44-504fab176c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10027, 33)\n",
      "(10027,)\n"
     ]
    }
   ],
   "source": [
    "X = np.concatenate((asteroid_cat_encoded, asteroid_num_scaled), axis=1)\n",
    "X = np.delete(X, 2, 1)\n",
    "y = asteroid_cat_encoded[:,2]\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d528090d-ad59-40b9-bf1c-dbdd05470a67",
   "metadata": {},
   "source": [
    "# Training and test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c72da619-a5fc-4da9-a7ee-4ac824b3b5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61171946-df6f-4cdc-8ccb-813f5bdf4a82",
   "metadata": {},
   "source": [
    "# Logistic Regression Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0453953-5f1c-41f6-af2c-035730ae17ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9506480558325024\n",
      "Micro F1 scorre:  0.9506480558325024\n",
      "Macro F1 score:  0.6539133467541507\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def logistic_regression():\n",
    "    # Initialization and fitting logisitic regression\n",
    "    log_reg = LogisticRegression(solver=\"newton-cg\", random_state=42)\n",
    "    log_reg.fit(X_train, y_train)\n",
    "\n",
    "    #Make predictions\n",
    "    y_pred = log_reg.predict(X_test)\n",
    "\n",
    "    #Model metrics\n",
    "    log_reg_accuracy = accuracy_score(y_test, y_pred)\n",
    "    log_reg_microf1 = f1_score(y_test, y_pred, average=\"micro\")\n",
    "    log_reg_macrof1 = f1_score(y_test, y_pred, average=\"macro\")\n",
    "\n",
    "    return log_reg, log_reg_accuracy, log_reg_microf1, log_reg_macrof1\n",
    "\n",
    "log_reg, log_reg_accuracy, log_reg_microf1, log_reg_macrof1 = logistic_regression()\n",
    "\n",
    "print(\"Accuracy: \", log_reg_accuracy)\n",
    "print(\"Micro F1 scorre: \", log_reg_microf1)\n",
    "print(\"Macro F1 score: \", log_reg_macrof1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb418c4",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ad64566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9835493519441675\n",
      "Micro F1 scorre:  0.9835493519441675\n",
      "Macro F1 score:  0.8244815821560008\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "def randomForest():\n",
    "    rndFr = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, random_state=42)\n",
    "    rndFr.fit(X_train, y_train)\n",
    "    y_pred_rf = rndFr.predict(X_test)\n",
    "    rndFr_accuracy = accuracy_score(y_test, y_pred_rf)\n",
    "    rndFr_microf1 = f1_score(y_test, y_pred_rf, average=\"micro\")\n",
    "    rndFr_macrof1 = f1_score(y_test, y_pred_rf, average=\"macro\")\n",
    "    return rndFr, rndFr_accuracy, rndFr_microf1, rndFr_macrof1\n",
    "\n",
    "rndFr, rndFr_accuracy, rndFr_microf1, rndFr_macrof1 = randomForest()\n",
    "\n",
    "print(\"Accuracy: \", rndFr_accuracy)\n",
    "print(\"Micro F1 scorre: \", rndFr_microf1)\n",
    "print(\"Macro F1 score: \", rndFr_macrof1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16264421-359e-47fe-b660-371d37860761",
   "metadata": {},
   "source": [
    "# Multi Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c3cd45c-116e-4cc1-a7f5-58cacda3dab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "251/251 [==============================] - 1s 2ms/step - loss: -117209145344.0000 - accuracy: 0.1790 - val_loss: -836305485824.0000 - val_accuracy: 0.1795\n",
      "Epoch 2/30\n",
      "251/251 [==============================] - 0s 2ms/step - loss: -15368809611264.0000 - accuracy: 0.1798 - val_loss: -50183958167552.0000 - val_accuracy: 0.1795\n",
      "Epoch 3/30\n",
      "251/251 [==============================] - 0s 2ms/step - loss: -200617351970816.0000 - accuracy: 0.1798 - val_loss: -434655924846592.0000 - val_accuracy: 0.1795\n",
      "Epoch 4/30\n",
      "251/251 [==============================] - 0s 2ms/step - loss: -1056807036387328.0000 - accuracy: 0.1798 - val_loss: -1856318688198656.0000 - val_accuracy: 0.1795\n",
      "Epoch 5/30\n",
      "251/251 [==============================] - 0s 1ms/step - loss: -3581921925791744.0000 - accuracy: 0.1798 - val_loss: -5493437654630400.0000 - val_accuracy: 0.1795\n",
      "Epoch 6/30\n",
      "251/251 [==============================] - 0s 2ms/step - loss: -9225334201253888.0000 - accuracy: 0.1798 - val_loss: -12804456076804096.0000 - val_accuracy: 0.1795\n",
      "Epoch 7/30\n",
      "251/251 [==============================] - 0s 2ms/step - loss: -19477369597198336.0000 - accuracy: 0.1798 - val_loss: -25615174215925760.0000 - val_accuracy: 0.1795\n",
      "Epoch 8/30\n",
      "251/251 [==============================] - 0s 2ms/step - loss: -36405774888468480.0000 - accuracy: 0.1798 - val_loss: -45991003726807040.0000 - val_accuracy: 0.1795\n",
      "Epoch 9/30\n",
      "251/251 [==============================] - 0s 2ms/step - loss: -62567013393891328.0000 - accuracy: 0.1798 - val_loss: -76490309525045248.0000 - val_accuracy: 0.1795\n",
      "Epoch 10/30\n",
      "251/251 [==============================] - 0s 1ms/step - loss: -101523199992266752.0000 - accuracy: 0.1798 - val_loss: -119955747927752704.0000 - val_accuracy: 0.1795\n",
      "Epoch 11/30\n",
      "251/251 [==============================] - 0s 1ms/step - loss: -154009882192773120.0000 - accuracy: 0.1798 - val_loss: -178870596011032576.0000 - val_accuracy: 0.1795\n",
      "Epoch 12/30\n",
      "251/251 [==============================] - 0s 2ms/step - loss: -225780881652973568.0000 - accuracy: 0.1798 - val_loss: -256181330188959744.0000 - val_accuracy: 0.1795\n",
      "Epoch 13/30\n",
      "251/251 [==============================] - 0s 2ms/step - loss: -315704972706578432.0000 - accuracy: 0.1798 - val_loss: -354856073510780928.0000 - val_accuracy: 0.1795\n",
      "Epoch 14/30\n",
      "251/251 [==============================] - 0s 1ms/step - loss: -431637547458756608.0000 - accuracy: 0.1798 - val_loss: -478475334052741120.0000 - val_accuracy: 0.1795\n",
      "Epoch 15/30\n",
      "251/251 [==============================] - 0s 1ms/step - loss: -577582597761138688.0000 - accuracy: 0.1798 - val_loss: -631541818089013248.0000 - val_accuracy: 0.1795\n",
      "Epoch 16/30\n",
      "251/251 [==============================] - 0s 2ms/step - loss: -749764057086558208.0000 - accuracy: 0.1798 - val_loss: -814585077907324928.0000 - val_accuracy: 0.1795\n",
      "Epoch 17/30\n",
      "251/251 [==============================] - 0s 2ms/step - loss: -962330922097573888.0000 - accuracy: 0.1798 - val_loss: -1034393670096257024.0000 - val_accuracy: 0.1795\n",
      "Epoch 18/30\n",
      "251/251 [==============================] - 0s 2ms/step - loss: -1211923222698655744.0000 - accuracy: 0.1798 - val_loss: -1293960808903999488.0000 - val_accuracy: 0.1795\n",
      "Epoch 19/30\n",
      "251/251 [==============================] - 0s 2ms/step - loss: -1508096195971514368.0000 - accuracy: 0.1798 - val_loss: -1597661313658519552.0000 - val_accuracy: 0.1795\n",
      "Epoch 20/30\n",
      "251/251 [==============================] - 0s 1ms/step - loss: -1854776610255798272.0000 - accuracy: 0.1798 - val_loss: -1951138496508854272.0000 - val_accuracy: 0.1795\n",
      "Epoch 21/30\n",
      "251/251 [==============================] - 0s 2ms/step - loss: -2252742370028158976.0000 - accuracy: 0.1798 - val_loss: -2357718517567979520.0000 - val_accuracy: 0.1795\n",
      "Epoch 22/30\n",
      "251/251 [==============================] - 0s 2ms/step - loss: -2691247948150341632.0000 - accuracy: 0.1798 - val_loss: -2817328946507415552.0000 - val_accuracy: 0.1795\n",
      "Epoch 23/30\n",
      "251/251 [==============================] - 0s 2ms/step - loss: -3231826963115016192.0000 - accuracy: 0.1798 - val_loss: -3346341923781607424.0000 - val_accuracy: 0.1795\n",
      "Epoch 24/30\n",
      "251/251 [==============================] - 0s 2ms/step - loss: -3791883551687835648.0000 - accuracy: 0.1798 - val_loss: -3935985820502065152.0000 - val_accuracy: 0.1795\n",
      "Epoch 25/30\n",
      "251/251 [==============================] - 0s 2ms/step - loss: -4475147015222722560.0000 - accuracy: 0.1798 - val_loss: -4601449015416979456.0000 - val_accuracy: 0.1795\n",
      "Epoch 26/30\n",
      "251/251 [==============================] - 0s 2ms/step - loss: -5202628988136062976.0000 - accuracy: 0.1798 - val_loss: -5343038272270499840.0000 - val_accuracy: 0.1795\n",
      "Epoch 27/30\n",
      "251/251 [==============================] - 0s 2ms/step - loss: -5986089297042735104.0000 - accuracy: 0.1798 - val_loss: -6168852318834917376.0000 - val_accuracy: 0.1795\n",
      "Epoch 28/30\n",
      "251/251 [==============================] - 0s 2ms/step - loss: -6930354281023275008.0000 - accuracy: 0.1798 - val_loss: -7092859900585312256.0000 - val_accuracy: 0.1795\n",
      "Epoch 29/30\n",
      "251/251 [==============================] - 0s 2ms/step - loss: -7896491300059348992.0000 - accuracy: 0.1798 - val_loss: -8103893827674177536.0000 - val_accuracy: 0.1795\n",
      "Epoch 30/30\n",
      "251/251 [==============================] - 0s 2ms/step - loss: -9070002809163743232.0000 - accuracy: 0.1798 - val_loss: -9227771182877507584.0000 - val_accuracy: 0.1795\n",
      "63/63 [==============================] - 0s 879us/step - loss: -9227771182877507584.0000 - accuracy: 0.1795\n",
      "63/63 [==============================] - 0s 941us/step\n",
      "MLP Test Accuracy: 0.1794616151545364\n",
      "MLP Micro F1 Score: 0.17946161515453637\n",
      "MLP Macro F1 Score: 0.0338123415046492\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def MLP():\n",
    "\n",
    "    learning_rate = 0.01\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    \n",
    "    # Define the MLP\n",
    "    model = keras.models.Sequential([\n",
    "        keras.layers.Dense(128, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "        keras.layers.Dropout(0.3),\n",
    "        keras.layers.Dense(128, activation=\"relu\"),\n",
    "        keras.layers.Dropout(0.3),\n",
    "        keras.layers.Dense(64, activation=\"relu\"),\n",
    "        keras.layers.Dropout(0.3),\n",
    "        keras.layers.Dense(32, activation=\"relu\"),\n",
    "        keras.layers.Dropout(0.3),\n",
    "        keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss=\"binary_crossentropy\",\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=[\"accuracy\"])\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(X_train, y_train, epochs=30, batch_size=32,\n",
    "                        validation_data=(X_test, y_test))\n",
    "\n",
    "    # Evaluate the model\n",
    "    model.evaluate(X_test, y_test)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred.round(2)\n",
    "\n",
    "    # Calculate scores\n",
    "    mlp_accuracy = accuracy_score(y_test, y_pred)\n",
    "    mlp_microf1 = f1_score(y_test, y_pred, average=\"micro\")\n",
    "    mlp_macrof1 = f1_score(y_test, y_pred, average=\"macro\")\n",
    "\n",
    "    return model, history, mlp_accuracy, mlp_microf1, mlp_macrof1\n",
    "\n",
    "mlp_model, mlp_history, mlp_accuracy, mlp_microf1, mlp_macrof1 = MLP()\n",
    "\n",
    "print(\"MLP Test Accuracy:\", mlp_accuracy)\n",
    "print(\"MLP Micro F1 Score:\", mlp_microf1)\n",
    "print(\"MLP Macro F1 Score:\", mlp_macrof1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113b66d5-32a4-41d5-91a0-5d5cf4a10e53",
   "metadata": {},
   "source": [
    "# XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "897cf7c1-391e-4733-a23d-8f8bd68b1194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Accuracy: 0.9925224327018943\n",
      "XGBoost Micro F1 Score: 0.9925224327018943\n",
      "XGBoost Macro F1 Score: 0.8677746762929767\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "def xgboost_clf():\n",
    "    # Initialize XGBoost\n",
    "    xgb_clf = xgb.XGBClassifier(objective='binary:logistic', random_state=42)\n",
    "\n",
    "    # Train the model\n",
    "    xgb_clf.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = xgb_clf.predict(X_test)\n",
    "\n",
    "    # Model metrics\n",
    "    xgb_accuracy = accuracy_score(y_test, y_pred)\n",
    "    xgb_microf1 = f1_score(y_test, y_pred, average=\"micro\")\n",
    "    xgb_macrof1 = f1_score(y_test, y_pred, average=\"macro\")\n",
    "\n",
    "    return xgb_clf, xgb_accuracy, xgb_microf1, xgb_macrof1\n",
    "\n",
    "xgb_model, xgb_accuracy, xgb_microf1, xgb_macrof1 = xgboost_clf()\n",
    "\n",
    "print(\"XGBoost Accuracy:\", xgb_accuracy)\n",
    "print(\"XGBoost Micro F1 Score:\", xgb_microf1)\n",
    "print(\"XGBoost Macro F1 Score:\", xgb_macrof1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922cd839-a50c-4b98-9354-0a96d02a35ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
